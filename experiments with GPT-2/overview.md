I read a blog post by the [open.ai](https://openai.com/) group called [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/).

Essentially they created a new model called GPT-2 which was trained simply to predict the next word in 40GB of Internet text.

### How was the model trained?

>GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. 

### What is the model's objective?

> GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text.

### So how does it work?

> GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. 

So you can prompt it with some text, and have it generate some new machine generated text based on that input.

### Examples

